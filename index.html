
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Go 2 Class 4 Me</title>

    <!-- Bootstrap core CSS -->
    <link href="dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <!-- <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

    <!-- Custom styles for this template -->
    <link href="starter-template.css" rel="stylesheet">

    <!-- Just for debugging purposes. Don't actually copy these 2 lines! -->
    <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->
    <!-- <script src="../../assets/js/ie-emulation-modes-warning.js"></script> -->

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>

  <body>

    <nav class="navbar navbar-inverse navbar-fixed-top">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>

          <a class="navbar-brand" href="#">Go 2 Class 4 Me</a>
        </div>
        <div id="navbar" class="collapse navbar-collapse">
          <ul class="nav navbar-nav">
            <li class="active"><a href="#">Home</a></li>
            <li><a href="#intro">Introduction</a></li>
            <li><a href="#obj">Project Objective</a></li>
            <li><a href="#design">Design</a></li>
            <li><a href="#testing">Testing</a></li>
            <li><a href="#result">Result</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>

    <div class="container">

      <div class="starter-template">
        <h1>ECE 5725 Project : Go2Class4Me Robot</h1>
        <p class="lead">Created by: Christine Ahn (cya6), Sophie He (bh377)<br>
        Date : December 14, 2019 </p>
      </div>

      <hr>
      <div class="center-block">
          <iframe width="640" height="360" src="https://www.youtube.com/embed/9HZo7vOw_Ks" frameborder="0" allowfullscreen></iframe>
          <h4 style="text-align:center;">Demonstration Video</h4>
      </div>

      <hr id="intro">

      <div style="text-align:center;">
              <h2>Introduction</h2>
              <p style="text-align: left;padding: 0px 30px;">
                One day at Cornell University, two students, Christine and Sophie were wondering what they should do for their final project in their Embedded OS class. While walking to their classes, they realized, "Ah, winter is coming" and there, an idea popped in both of their minds. What if something could go to class in their stead? Instead of walking to class in the cold weather, especially when it starts to snow, we could just virtually be there through a robot? And even better, on days where we are just too tired to get out of bed, this robot could still go to class for us!
                <br><br>
                For any time, whether you are too lazy to get out of bed or the slushy snow outside is too serious of a slipping hazard, here we introduce and propose a wirelessly controlled robot that can go to class for students. While in your bed, all you need to do is open the app, and then move your phone to move the robot!
              </p>
      </div>

    <hr id='obj'>

      <div class="row">
          <div class="col-md-8" style="font-size:18px;">
          <h2>Project Objective:</h2>
          <ul>
              <li>Control the robot wirelessly using a phone.</li>
              <li>See and hear what the robot sees and hears through the phone.</li>
              <li>Feel that one could possibly have the robot go to class for them.</li>
          </ul>
          </div>
      </div>

    <hr id='design'>

      <div style="text-align:center;">
          <h2>Design</h2>
          <br>
          <img class="img-rounded" src="pics/system_diagram.JPG" alt="system diagram" width="541" height="373">
          <p><i>Diagram of the overall system of our project </i></p>
          <br>
          <p style="text-align: left;padding: 0px 30px;">
              Our project consists of a robot that can move forward, backward, turn left, and turn right depending on inputs from a wireless accelerometer on a phone. The robot can also capture audio through a microphone and video imaging using a camera, both configured onto the robot. The image and audio are transmitted to a phone via WiFi.
              <br>
              By the end of our 5 weeks, we have successfully accomplished the following :
            <ul style="text-align: left;padding: 0px 30px;">
              <li>Implemented socket programming for communication between the robot and phone</li>
              <li>Built robot using materials provided in Lab 3</li>
              <li>Configured servos for the robot's motion through PWM signals</li>
              <li>Set up Camera and Microphone and livestreamed it online to a website</li>
              <li>Set up phone's accelerometer information to send to pi</li>
              <li>Created an application for the phone to diplay the livestream video and audio</li>
            </ul>
          </p>
          <br><br>

          <img class="img-rounded" src="pics/robot_right.png" alt="Generic placeholder image" style="width:80%;">
          <div class="robo_images" style="text-align:center;">
          <img class="img-rounded" src="pics/robot_front.jpg" alt="robot_front" width="240" height="240">
          <img class="img-rounded" src="pics/robot_left.jpg" alt="robot_left" width="280" height="240">
          <img class="img-rounded" src="pics/robot_back.jpg" alt="robot_back" width="240" height="240">
          </div>
      </div>

    <hr id='camera'>

      <div style="text-align:center;">
              <h2>Camera</h2>
              <p style="text-align: left;padding: 0px 30px;">
              	We used the Raspberry Pi camera and installed it following this
                <a href="https://thepihut.com/blogs/raspberry-pi-tutorials/16021420-how-to-install-use-the-raspberry-pi-camera">tutorial</a>.

                We first connected the camera in the slot in the Raspberry Pi,
                which is between the HDMI and Ethernet ports,
                with the silver connectors facing the HDMI port.

                Then we ran <code> sudo raspi-config</code> in the terminal
                to enable the camera. This is achieved by selecting
                <strong> 5 Interfacing Options -> P1 Camera -> Yes</strong>.

                If the camera option is not available, then an update needs
                to be made, which in this case, <code>sudo apt-get update</code>
                and <code>sudo apt-get upgrade</code> would need to be run in
                the terminal window.

                After enabling the camera,
                we rebooted (<code>sudo reboot</code>)the Raspberry Pi.

                <br><br>

                We played around with the camera by taking photos (<code>raspistill -o image.jpg</code>)
                and videos (<code>raspivid -o video.h264 -t 10000</code>).
                This can be viewed using <code> gpicview image.jpg </code>
                for photos and <code> omxplayer -o hdmi /path/to/filename.mp4 </code>
                for videos.

                <br><br>

                For livestreaming video, we followed the following
                <a href="https://randomnerdtutorials.com/video-streaming-with-raspberry-pi-camera/?fbclid=IwAR2VapDATNAdWZw4prG8uiNBKwqL0IXu7jidJOO81o0FDb9PHFSmfcW6Odc">tutorial</a>
                and used this <a href="https://raw.githubusercontent.com/RuiSantosdotme/Random-Nerd-Tutorials/master/Projects/rpi_camera_surveillance_system.py">code</a>
                to stream the live video feed to a website http://&ltYour_Pi_IP_Address&gt:8000.

                We accessed the video streaming through an Android phone that
                is connected to the same WiFi network, enabling the user on the
                phone end to view the live feed of the camera without being in
                the same area as the camera. We were able to get this to work with
                very little latency.

                <br><br>
              </p>
              <div align="center">
                <img class="img-rounded" src="pics/camera1.jpg" alt="testing camera" width="240" height="240">
                <p><i>Raspberry Pi camera live feed showing up on phone</i></p>
              </div>

      </div>

      <hr id='audio'>

      <div style="text-align:center;">
              <h2>Audio</h2>
              <p style="text-align: left;padding: 0px 30px;">
              We used this
              <a href="https://www.amazon.com/eBerry-Microphone-Adjustable-Compatible-Recording/dp/B00UZY2YQE">microphone</a>
              which connected to the Raspberry Pi via USB.
              To check the USB connections, we entered <code>lsusb</code> in the
              terminal. To test out the audio output, we used the
              <code>arecord</code> and <code>aplay</code> commands and followed
              the following steps from this
              <a href="https://blog.mutsuda.com/raspberry-pi-into-an-audio-spying-device-7a56e7a9090e#.fr4l82xek">tutorial</a>.

              <br><br>

              We loaded the audio module <code>sudo modprobe snd_bcm2835</code>
              and tested the output by recording some sound with
              <code>arecord -D plughw:1,0 test.wav</code> and exited the command
              when finished recording (control+c). To hear the sound back, we used
              <code>aplay test.wav</code>. To adjust the sound of the microphone
              we used <code>alsamixer</code> and saved the settings using
              <code>sudo alsactl store</code>. We found that the sound was
              generally a bit quiet, so turning the microphone volume high
              was a good idea.

              <br><br>

              To make this sound play on the phone, we had to find a way to
              make it stream to a website. We found that this would be possible
              using <a href="http://www.darkice.org/">DarkIce</a> and
              <a href="http://icecast.org/">Icecast</a> packages through the
              following <a href="https://maker.pro/raspberry-pi/projects/how-to-build-an-internet-radio-station-with-raspberry-pi-darkice-and-icecast">tutorial</a>.
              DarkIce is allows one to record audio and stream it by
              encoding the audio and sending the mp3 stream to a streaming server (in our
              case, Icecast). Icecast is a audio streaming server, which we utlize
              to run the audio stream.

              We entered the following three lines in the terminal.

              <br><br>

              <pre align="left"><code>
    wget https://github.com/x20mar/darkice-with-mp3-for-raspberry-pi/blob/master/darkice_1.0.1-999~mp3+1_armhf.deb?ra
    w=true mv darkice_1.0.1-999~mp3+1_armhf.deb?raw=true darkice_1.0.1-999~mp3+1_armhf.deb
    sudo apt-get install libmp3lame0 libtwolame0
    sudo dpkg -i darkice_1.0.1-999~mp3+1_armhf.deb
              </code></pre>

              <br><br>
              <p style="text-align: left;padding: 0px 30px;">
                Next, we downloaded Icecast <code>sudo apt-get install icecast2</code>.
                On the window that pops up, we selected "Yes" to configure Icecast2.
                We used the default hostname "localhost", as well as the default
                source password and relay password.


              <br><br>


              After the installation of Icecast2, we made a config file
              <code>sudo nano darkice.cfg</code> for DarkIce.

              In the file, we pasted the following configurations:
              </p>

              <pre align="left"><code>
      [general]
      duration        = 0      # duration in s, 0 forever
      bufferSecs      = 1      # buffer, in seconds
      reconnect       = yes    # reconnect if disconnected

      [input]
      device          = plughw:1,0 # Soundcard device for the audio input
      sampleRate      = 44100   # sample rate 11025, 22050 or 44100
      bitsPerSample   = 16      # bits
      channel         = 2       # 2 = stereo

      [icecast2-0]
      bitrateMode     = cbr       # constant bit rate ('cbr' constant, 'abr' average)
      #quality         = 1.0       # 1.0 is best quality (use only with vbr)
      format          = mp3       # format. Choose 'vorbis' for OGG Vorbis
      bitrate         = 320       # bitrate
      server          = localhost # or IP
      port            = 8000      # port for IceCast2 access
      password        = hackme    # source password for the IceCast2 server
      mountPoint      = rapi.mp3  # mount point on the IceCast2 server .mp3 or .ogg
      name            = Raspberry Pi
              </pre></code>

              <p style="text-align: left;padding: 0px 30px;">
              We created another file called <code>darkice.sh</code>
              and used the command <code>sudo nano darkice.sh</code>
              to paste the following configurations.

              </p>

              <br><br>
              <pre align="left"><code>
    #!/bin/bash <br>
    sudo /usr/bin/darkice -c /home/pi/darkice.cfg
              </pre></code>

              <p style="text-align: left;padding: 0px 30px;">
              To make this file executable, we ran <code>sudo chmod 777 /home/pi/darkice.sh</code>
              and ran the following three commands to start the Icecast2 service.
              </p>


              <pre align="left"><code>
    sudo service icecast2 start <br>
    sudo chmod 777 /home/pi/darkice.sh <br>
    sudo service icecast2 start
              </pre></code>

              <p style="text-align: left;padding: 0px 30px;">
              We then entered the command <code>select-editor</code> and selected
              2 for the nano editor (<code>crontab -e</code>). In the bottom
              of the text file that appeared, we pasted the following line,
              which alows the Raspberry Pi to wait for 10 seconds before it
              starts the stream to initialize:
              <code>@reboot sleep 10 && sudo /home/pi/darkice.sh</code>.

              <br><br>

              After a reboot (<code>sudo reboot</code>) of the Raspberry Pi, we
              used the command <code>ifconfig</code> to check the IP address of the
              Raspberry Pi (if it has changed), and can now go to the website
              <strong>http://Your_Pi_IP_Address:8000</strong> to get to the Icecast2 page.
              From there, we can click on the M3U button on the top right corner
              and it will take us to a livestream feed of the audio.
              By going to <strong>http://Your_Pi_IP_Address:8000/rapi.mp3</strong>,
              we could directly play the stream. We added this to the Raspberry
              Pi camera code, so that the livestream audio could display on the
              website as well.

              <br><br>

              One problem we encountered in our beginning stages of installing DarkIce
              was when we tried to use the wheezy debian
              release instead of the buster one. This happened because many
              tutorials online for audio streaming were older, and therefore used
              and older (first) version of the debian release.
              It caused one of our SD cards
              to no longer be able to read or write.

              <br><br>

              Another issue we had with the USB microphone was that the sound
              was inconsistent. Sometimes when we would test it, the sound would
              be clear, and other times the noise would be so loud that other
              outside sounds could not be heard. We found that talking closer
              to the microphone generally would produce clearer results. One way
              improve the microphone was suggested to us to use an iPhone earbud
              microphone, and this can be possibly implemented in the future.

              </p>
          </p>
      </div>

      <hr id='androidscreen'>

      <div style="text-align:center;">
          <h2>Android Application</h2>
<br>
          <p>
            <i>Video from the phone's point of view, controlling the robot to make it enter the lab room. </i>
          </p>

          <div style="text-align: center">
            <iframe width="534" height="300" src="https://www.youtube.com/embed/nMi6LH04kw4" frameborder="0" allowfullscreen=""></iframe>
          </div>
<br>
          <p style="text-align: left;padding: 0px 30px;">
            We were able to get the livestream video and audio by using webView on the android phone through this <a href="https://medium.com/@stevesohcot/andriod-studio-webview-tutorial-4651701d7d1a">tutorial</a>. We initially tried to use videoView and vitamio but could not get that to work.
            <br><br>
            As in the tutorial, we initialized a WebView variable and set it up in the onCreate section as such :
          </p>
          <div align="left">
            <pre><code>
      public class MainActivity extends AppCompatActivity implements SensorEventListener {
        private WebView webview ;
        ...
        @Override
        protected void onCreate (Bundle savedInstanceState) {
        ...
        webview =(WebView)findViewById(R.id.webView);

        webview.setWebViewClient(new WebViewClient());
        webview.getSettings().setJavaScriptEnabled(true);
        webview.getSettings().setDomStorageEnabled(true);
        webview.setOverScrollMode(WebView.OVER_SCROLL_NEVER);
        webview.loadUrl(videoURL);

       }
      }
            </code></pre>
          </div>
          <p style="text-align: left;padding: 0px 30px;">
            Then, in the activity_main.xml file (under the res/layouts folder), one must also remember to put in a WebView object so that the findViewById" line can find the view and load the website onto that view.
            <br><br>
          </p>
          <h4 style="text-align: left;padding: 0px 30px;">Bugs</h4>
          <p style="text-align: left;padding: 0px 30px;">
            If the phone still cannot access the website, one should check their AndroidManifext.xml file (under the manifests folder) if it has the
            <b>&lt;uses-permission android:name="android.permission.INTERNET"/&gt;</b> line included. We also found a bug where the phone could not access the website if it was not under the same Cornell Wifi as the robot. So, one should make sure that if the Pi is livestreaming the camera and audio information online under the Cornell Wifi that the phone is also using the Cornell Wifi (not data).
          </p>

      </div>

      <hr id='control'>

      <div style="text-align:center;">
              <h2>Controlling the Robot with a Phone</h2>
              <div align="center">
              <img class="img-rounded" src="pics/accelerometer.png" alt="Phone coordinate system for sensors" width="240" height="240">
              </div>
              <br>
              <p style="text-align: left;padding: 0px 30px;">
              To control the robot wirelessly, we needed to create a socket to send accelerometer information from the phone (the server) to be received by the Raspberry Pi (the client). All code can be either seen in the Code Appendix section or on the <a href="https://github.com/cya6/WifiRobot">GitHub repository</a>.
              </p>
              <h3>1. Configuring the Phone's Accelerometer </h3>
              <p style="text-align: left;padding: 0px 30px;">
                We configured the phone's accelerometer using AndroidStudio to create an application.
                To configure and use the accelerometer within the phone through the app, we followed some tutorials that showed us how to set up and print out the accelerometer values:
                <br><br>
                <a href="https://www.youtube.com/watch?v=pkT7DU1Yo9Q&t=682s">Set up accelerometer</a>
                <br>
                <a href="https://www.youtube.com/watch?v=Rda_5s4rObQ">Print out accelerometer values</a>
                <br><br>

                First, we used an empty template through Android Studio. Then, we followed the tutorial, adding the SensorManager and Sensor variables to the MainActivity class. Next, inside the onCreate function, we configure the SensorManager and Sensor to get information from the phone's accelerometer. Although the tutorial uses the standard default accelerometer, we chose to use the rotation vector accelerometer as it seemed more able to sense turning the phone more easily. An overview of the available sensors on the phone can be seen <a href="https://developer.android.com/guide/topics/sensors/sensors_motion.html">here</a>.
                <br><br>
                We also then configure the SensorManager to allow us listen to the information from the phone's accelerometer. Next, by using the sensor class, there are two functions that must be included in the MainActivity file : onAccuracyChanged and onSensorChanged. For our purposes, we only use onSensorChanged since we only wanted to read the x, y, and z values output by the accelerometer. In the onSensorChanged function, it takes in a sensor event which then contains the x, y, and z values from the sensor. We then save this information by saving the read values to a string.
                <br>
              </p>
                <div align = "left">
                <pre><code>
public class MainActivity extends AppCompatActivity implements SensorEventListener {

    private static final String TAG = "MainActivity";
    private SensorManager sensorManager;
    private Sensor accelerometer;

    String x,y,z;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_main);

        sensorManager = (SensorManager) getSystemService(Context.SENSOR_SERVICE);
        accelerometer = sensorManager.getDefaultSensor(Sensor.TYPE_ROTATION_VECTOR);

        sensorManager.registerListener(MainActivity.this,accelerometer, SensorManager.SENSOR_DELAY_NORMAL);
    }


    @Override
    public void onAccuracyChanged(Sensor sensor, int accuracy) {

    }

    @Override
    public void onSensorChanged(SensorEvent event) {
        Log.d(TAG, "X: " + event.values[0] + " |Y: " + event.values[1]);

        x = "xValue: "+event.values[0];
        y = "yValue: "+event.values[1];
        z = "zValue: "+event.values[2];

    }
                </code></pre>
              </div>
              <p style="text-align: left;padding: 0px 30px;">
                To see the accelerometer values for yourself, you can use Log.d to print out the values to Android Studio's Logcat. Next, after setting up the accelerometer on the phone, we needed to send it to the RPi.
              </p>
              <h3>2. Sending information to the RPi with Socket Programming </h3>

              <h4 style="text-align: left; padding: 0px 30px">Server side ( Android Application )</h4>
              <p style="text-align: left;padding: 0px 30px;">
                We used socket programming to send information from the phone to the RPi. The phone was set up as the server by following the information <a href="http://androidsrc.net/android-client-server-using-sockets-server-implementation/">here</a>.
                <br><br>
                Following this tutorial, after setting up the server as they did, we changed some parts of it to send to the client the accelerometer information continuously. In MainActivity.java, we initialize the server in onCreate. We also include a function that will destroy the server when the application is closed.
                <div align="left">
                  <pre><code>
        public class MainActivity extends AppCompatActivity implements SensorEventListener {
          Server server ;
          ...

          @Override
          protected void onCreate(Bundle savedInstanceState) {
            ...
            server = newServer(this);
            ...
          }
          ...

          @Override
          protected void onDestroy() {
              super.onDestroy();
              server.onDestroy();
          }
        }
                  </code></pre>
                </div> <p>
              <p style="text-align: left;padding: 0px 30px;">
                In Server.java, in the run () code section, we see that it has an outputStream that it sends to the Client, in our case the RPi. We change the section to continuously in a while ( true ) loop, print to the outputStream the x, y, and z values recorded in MainActivity. With this, the server has been set up to send across the accelerometer information. We also modified the tutorial code to not print out anything on the application screen.
              </p>
              <div align="left">
                <pre><code>
        @Override
        public void run() {
            OutputStream outputStream ;

            try {
                outputStream = hostThreadSocket.getOutputStream();
                PrintStream printStream = new PrintStream(outputStream);
                while (true) {
                    printStream.println(activity.x + "\n" +
                            activity.y + "\n" +
                            activity.z  + "\n");
                    sleep(1000);
                }
                printStream.close();

            }
            catch (IOException e) {
                e.printStackTrace();
                message += "Something wrong! " + e.toString() + "\n";
            }
            catch (InterruptedException e) {
                e.printStackTrace();
                message += "Something wrong! " + e.toString() + "\n";
            }

            activity.runOnUiThread(new Runnable() {
                @Override
                public void run() {
                    activity.msg.setText(message);
                }
            });
        }
                </code></pre>
              </div>
              <br>
              <h4 style="text-align: left; padding: 0px 30px">Client side ( from the RPi point of view )</h4>
              <p style="text-align: left;padding: 0px 30px;">
                To set up the client code on the Raspberry Pi, we followed the "Echo Client" code in this <a href="https://realpython.com/python-sockets/">tutorial</a>.
                <br>
                The HOST value was set to the IP address of the android phone ( which can be found by clicking on the wifi button on your phone ).
                We also made sure it was using the same port as was used on the application side in Server.java. A change we made however was to continuously receive data, decode it into ASCII characters, and then send that information to a FIFO. This FIFO is important as it is what we will use to control the robot with the accelerometer. The code can be seen in our <a href="https://github.com/cya6/WifiRobot">repository</a> under client.py.
              </p>

              <h3>3. Moving the robot with the accelerometer information</h3>
              <p style="text-align: left;padding: 0px 30px;">
                Finally, to move the robot, we read from the FIFO and based on the values from the accelerometer, decide to turn left, right, forward, or backwards. Code for this is found in our repository, under <a href="https://github.com/cya6/WifiRobot/blob/master/project/robot_control.py">robot_control.py</a>.
                <br><br>
                To allow the user to stand in any direction and start moving the robot, we first allow for a calibration period in the beginning where it will set the x and z values read to be the setting for which the user wishes to keep the robot still.
                <br>
                Next, after it is calibrated, it continues to read lines from the FIFO. We realized that when the phone (set horizontally) is tilted forward, the z value goes up by 0.1 and x value down by 0.1 from its steady state. To prevent from false forwards however, we set the code up so that it must detect this at least twice before it goes forward. Similarly, to go backwards, it detects that the z value goes down by 0.1 and x value up by 0.1 from its steady state. These threshold values were decided on after trial and error and may need to be adjusted if used on another device.
                We also use a buffer here before we actually decide to move the robot backwards. If neither of these conditions are detected, we keep the robot still.
                <br> <br>
                To go left or right, we depend solely on the x values read from the accelerometer. We also prevent the robot from turning if the user is going forward or backwards at that moment. To go right, we see if the x value goes down by 0.1 and to go left, if it goes up by 0.05. Similarly, this values were also found through trial and error and may need to be adjusted if run on another device. We also use a buffer here to detect more than 2 lefts or rights before the robot will actually turn right or left. If neither are detected, it will remain still.
                <br>

              </p>
      </div>

    <hr id='testing'>

      <div style="text-align:center;">
              <h2>Testing</h2>
              <p style="text-align: left;padding: 0px 30px;">
                First, we tested that our robot setup from Lab 3 was still working properly such that the robot could still move. This was important because we actually did find some errors with our servos: our left servo couldn't move. We tried various things such as replacing the batteries and switching out the servo. However, in the end, we realized it was a problem in the code and that we were setting up the wrong GPIO pin. 
                <br>
                Then, we tested our camera's configuration on the Raspberry Pi by first taking some sample still pictures. Similarly, we tested the audio's configuration by recording some short snippets using the tutorial. 
                <br>
                Next, after confirming that the camera and microphone were properly configured, we tested the livestream of the camera and audio by going on the website. We primarily checked if the latency between the moving on the camera and seeing it on the website was ok. The camera latency was pretty good but the audio latency was slow ( varied from 3 seconds to 20 seconds ). 
                <br>
                We tested the phone application by running it on the phone and seeing that we could see the camera and audio output well, similar to how we saw it on the desktop website. 
              </p>
                <img class="img-rounded" src="pics/testing_camera.jpg" alt="camera view on phone" width="350" height="480">
                <p><i>Testing the camera's view on the phone</i></p>
              <p style="text-align: left;padding: 0px 30px;">
                We tested moving the robot by running the code, and using the application on the phone to move it forward, backward, right, and left. We also tested using the robot at far distances, checking that the Wifi connection would work. However, in testing this, although the robot would still move we saw that the camera latency became very laggy and could not fix this.
                <br>
              </p>

          <div style="text-align: center">
            <iframe width="534" height="300" src="https://www.youtube.com/embed/djWUpxsrNRA" frameborder="0" allowfullscreen=""></iframe>
          </div>
          <p><i>Moving the robot with the phone</i></p>
      </div>


      <div style="text-align: center">
        <iframe width="534" height="300" src="https://www.youtube.com/embed/vVT-JkmS2Tc" frameborder="0" allowfullscreen=""></iframe>
      </div>


    <hr id='result'>

      <div style="text-align:center;">
              <h2>Result</h2>
              <p style="text-align: left;padding: 0px 30px;">
                In the end, we were able to achieve the overall goal of our project : to move the robot wirelessly with our phone. We were able to meet out goals of : 
                <ul style="text-align: left;padding: 0px 70px;">
                  <li>Learning how to socket program so that the RPi and phone could communicate </li>
                  <li>Configure and send accelerometer information from the phone to the RPi</li>
                  <li>Move the robot with the accelerometer information</li>
                  <li>Set up the RPi camera and audio and livestream it online </li>
                  <li>Have the phone be able to see and hear the livestream</li>
                </ul>
              </p>
                <p style="text-align: left;padding: 0px 30px;">
                Something we weren't able to achieve was using the VR headset in conjunction with our already established design. We decided to not use the VR headset in the end because of timing issues and were not sure if we could callibrate the accelerometer values well with the VR headset. We also decided that just using the phone was also okay and still let us see and hear what the robot sees and hears.
              </p>
      </div>

      <hr id='conclusion'>

      <div style="text-align:center;">
              <h2>Conclusions</h2>
              <p style="text-align: left;padding: 0px 30px;">
                Our project was able to successfully move the robot wirelessly with our phone and finish all project objectives. However, in the end, we discovered that the microphone used did not work well ( latency and sound issues ) and that using an earphone's microphone might've worked better. We also found that although Wifi should be reliable, there still existed some lagging once the robot became too far away. We guess that this may be because the IP address depending on where we are working on the robot ( despite being on the same Cornell Wifi ) changes. <br>
                Overall, we believe our project was successful in being able move wirelessly and allow someone to go enter a classroom without beng there themselves.
              </p>
      </div>

      <hr id='future work'>

      <div style="text-align:center;">
              <h2>Future Work</h2>
              <p style="text-align: left;padding: 0px 30px;">
                In the future, this project could be improved on by implementing a few more objectives:
                <ul style="text-align: left;padding: 0px 60px;">
                  <li>We could implement the VR headset and re-configure the thresholds for turning backward, forward, left, and right when the VR headset is in use. In using the VR headset, one could also try to configure or get a new camera that would allow the user to feel like they are the robot ( as VR view is different from regular flat camera view ). </li>
                  <li>We could fix the latency and audio issues of the microphone by using a different microphone. Nowadays, the microphones used on earphones ( like Apple's ) are good enough to filter out other noises and focus only on the speaker. We could've used and try to configure this microphone instead for better quality. </li>
                  <li>We could fix the robot architecture and design to be taller and allow for a more smoother and human-like view from the camera. Since the robot is small, the view that it sees is from the ground which doesn't actually let the user see the screen of any room ( unless the screen of the room is on a lower level compared to the robot ). The robot could be redesigned to be taller, and also firmly place the camera so that it does not move so much while the robot is moving. </li>
                </ul>
              </p>
      </div>

    <hr>

    <div class="row" style="text-align:center;">
          <h2>Work Distribution</h2>
          <div style="text-align:center;">
              <img class="img-rounded" src="pics/group.jpg" alt="Generic placeholder image" style="width:80%;">
              <h4>Project group picture</h4>
          </div>
          <div class="col-md-6" style="font-size:16px">
              <img class="img-rounded" src="pics/christine.jpg" alt="christine" width="240" height="240">
              <h3>Christine</h3>
              <p class="lead">cya6@cornell.edu</p>
              <p>Worked on the robot.</p>
          </div>
          <div class="col-md-6" style="font-size:16px">
              <img class="img-rounded" src="pics/sophie.jpg" alt="sophie" width="240" height="240">
              <h3>Sophie</h3>
              <p class="lead">bh377@cornell.edu</p>
              <p>Edit this sophie</p>
          </div>
      </div>

    <hr>
      <div style="font-size:18px">
          <h2>Parts List</h2>
          <ul>
              <li><a href="https://www.amazon.com/Raspberry-Pi-Camera-Module-Megapixel/dp/B01ER2SKFS">Raspberry Pi Camera V2 $25.00</a></li>
              <li><a href="https://www.amazon.com/eBerry-Microphone-Adjustable-Compatible-Recording/dp/B00UZY2YQE">Microphone $8</a></li>
              <li>Phone - Used student's existing android phone </li>
              <li>Raspberry Pi, LEDs, Resistors, Wires, Robot Parts- Provided in lab</li>
          </ul>
          <h3>Total: $33</h3>
      </div>
      <hr>
      <div style="font-size:18px">
          <h2>References</h2>
          <a href="https://picamera.readthedocs.io/">PiCamera Document</a><br>
          <a href="https://www.youtube.com/watch?v=pkT7DU1Yo9Q&t=682s">Accelerometer Tutorial 1
          </a><br>
          <a href="https://www.youtube.com/watch?v=Rda_5s4rObQ">Accelerometer Tutorial 2
          </a><br>
          <a href="https://developer.android.com/guide/topics/sensors/sensors_motion.html"> Android Sensor Information
          </a><br>
          <a href="http://androidsrc.net/android-client-server-using-sockets-server-implementation/"> Android Socket Programming
          </a><br>
          <a href="https://realpython.com/python-sockets/"> Python Socket Programming
          </a><br>
          <a href="https://medium.com/@stevesohcot/andriod-studio-webview-tutorial-4651701d7d1a"> WebView on Android Phone</a><br>


      </div>

    <hr>

      <div class="row">
              <h2>Code Appendix</h2>
              <h3><a href="https://github.com/cya6/WifiRobot/blob/master/project/robot_control.py">Robot Control Code</a> </h3>
              <h3><a href="https://github.com/cya6/WifiRobot/blob/master/project/client.py">Client Code</a> </h3>
              <h3><a href="https://github.com/cya6/WifiRobot/blob/master/project/rpi_camera.py">Camera Code</a> </h3>
              <h3><a href="https://github.com/cya6/WifiRobot/blob/master/project/rpi_camera.py">Phone Application Code</a> </h3>
              <h3><a href="https://raw.githubusercontent.com/RuiSantosdotme/Random-Nerd-Tutorials/master/Projects/rpi_camera_surveillance_system.py">Camera Web Streaming Code</a> </h3>

      </div>

    </div><!-- /.container -->




    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
    <script src="dist/js/bootstrap.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <!-- <script src="../../assets/js/ie10-viewport-bug-workaround.js"></script> -->
  </body>
</html>
